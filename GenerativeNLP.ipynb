{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "word_freqs = defaultdict(int)\n",
        "\n",
        "with open(\"input.txt\",\"r\") as file:\n",
        "  contents = file.read()\n",
        "\n",
        "corpus = contents\n",
        "\n",
        "for text in corpus:\n",
        "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    new_words = [word for word, offset in words_with_offsets]\n",
        "    for word in new_words:\n",
        "        word_freqs[word] += 1\n",
        "\n",
        "alphabet = []\n",
        "\n",
        "for word in word_freqs.keys():\n",
        "    for letter in word:\n",
        "        if letter not in alphabet:\n",
        "            alphabet.append(letter)\n",
        "alphabet.sort()\n",
        "\n",
        "vocab = [\"<|endoftext|>\"] + alphabet.copy()\n",
        "\n",
        "splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
        "\n",
        "def compute_pair_freqs(splits):\n",
        "    pair_freqs = defaultdict(int)\n",
        "    for word, freq in word_freqs.items():\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "        for i in range(len(split) - 1):\n",
        "            pair = (split[i], split[i + 1])\n",
        "            pair_freqs[pair] += freq\n",
        "    return pair_freqs\n",
        "\n",
        "pair_freqs = compute_pair_freqs(splits)\n",
        "\n",
        "for i, key in enumerate(pair_freqs.keys()):\n",
        "    print(f\"{key}: {pair_freqs[key]}\")\n",
        "    if i >= 5:\n",
        "        break\n",
        "\n",
        "best_pair = \"\"\n",
        "max_freq = None\n",
        "\n",
        "for pair, freq in pair_freqs.items():\n",
        "    if max_freq is None or max_freq < freq:\n",
        "        best_pair = pair\n",
        "        max_freq = freq\n",
        "\n",
        "\n",
        "def merge_pair(a, b, splits):\n",
        "    for word in word_freqs:\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "\n",
        "        i = 0\n",
        "        while i < len(split) - 1:\n",
        "            if split[i] == a and split[i + 1] == b:\n",
        "                split = split[:i] + [a + b] + split[i + 2 :]\n",
        "            else:\n",
        "                i += 1\n",
        "        splits[word] = split\n",
        "    return splits\n",
        "\n",
        "    vocab_size = 50\n",
        "\n",
        "    while len(vocab) < vocab_size:\n",
        "        pair_freqs = compute_pair_freqs(splits)\n",
        "        best_pair = \"\"\n",
        "        max_freq = None\n",
        "    for pair, freq in pair_freqs.items():\n",
        "        if max_freq is None or max_freq < freq:\n",
        "            best_pair = pair\n",
        "            max_freq = freq\n",
        "    splits = merge_pair(*best_pair, splits)\n",
        "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
        "    vocab.append(best_pair[0] + best_pair[1])\n",
        "\n",
        "def tokenize(text):\n",
        "    merges = {(\"Ġ\", \"t\"): \"Ġt\"}\n",
        "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
        "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
        "    for pair, merge in merges.items():\n",
        "        for idx, split in enumerate(splits):\n",
        "           i = 0\n",
        "           while i < len(split) - 1:\n",
        "               if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
        "                  split = split[:i] + [merge] + split[i + 2 :]\n",
        "               else:\n",
        "                  i += 1\n",
        "           splits[idx] = split\n",
        "\n",
        "    return sum(splits, [])\n",
        "\n"
      ],
      "metadata": {
        "id": "oUgQ2DTEvVZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "tokens = tokenizer.tokenize([\"pi\",\"phi\"])\n",
        "\n",
        "print(tokens)\n",
        "\n",
        "\n",
        "\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)\n",
        "decoded = tokenizer.decode(ids)\n",
        "print(decoded)"
      ],
      "metadata": {
        "id": "6t5jDKNbhUXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjEkEvittt4-"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "import string\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "SEQ_LEN = 4 #4\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 1e-3\n",
        "device = 'cpu'\n",
        "\n",
        "with open(\"input.txt\",\"r\") as file:\n",
        "  contents = file.read()\n",
        "\n",
        "letters = string.printable\n",
        "char_to_id = {k: i for i, k in enumerate(letters)}\n",
        "\n",
        "print(char_to_id)\n",
        "\n",
        "id_to_char = {i: k for i, k in enumerate(letters)}\n",
        "char_to_id['Ċ'] = len(char_to_id)\n",
        "id_to_char[len(id_to_char)] = 'Ċ'\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, contents, seq_len, char_to_id):\n",
        "        # List of IDs, each corresponding to a character in the sequence\n",
        "        self.contents = contents # Keep the contents as a string\n",
        "        # How long each sequence/token is\n",
        "        self.seq_len = seq_len\n",
        "        self.char_to_id = char_to_id\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        input_text = self.contents[i : i + self.seq_len]\n",
        "        output_text = self.contents[i + 1 : i + self.seq_len + 1]\n",
        "        input = torch.tensor([self.char_to_id[x] for x in input_text]) # Convert characters to IDs after slicing\n",
        "        output = torch.tensor([self.char_to_id[x] for x in output_text])\n",
        "        #embedding table\n",
        "        return input, output\n",
        "\n",
        "    # Length of dataset will be the number of tokens shifted by time step\n",
        "    def __len__(self):\n",
        "        return len(self.contents) - self.seq_len\n",
        "\n",
        "dataset = Dataset(contents, SEQ_LEN, char_to_id)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# %%\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.rnn = torch.nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
        "        # self.rnn = torch.nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        `x` is a tensor of shape (N/batch_size, L/seq_length, H_in/input_size)\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0) # Get the batch size of the input tensor, either 128 or 1\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
        "        out, hidden = self.rnn(x, hidden) # For RNN\n",
        "        # cell_state = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
        "        # out, hidden = self.rnn(x, (hidden, cell_state)) # For LSTM\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def predict(self, input: str, length: int, verbose=False) -> str:\n",
        "        result = input\n",
        "        input = input[-1]\n",
        "\n",
        "        for i in range(length):\n",
        "            if verbose:\n",
        "                print(\"Iteration\", i)\n",
        "            input = char_to_id[input]\n",
        "            # Convert to one-hot (shape: [100])\n",
        "            inp_vec = torch.nn.functional.one_hot(torch.tensor(input), num_classes=len(char_to_id))\n",
        "\n",
        "            # Add empty dimensions (shape: [1, 1, 100])\n",
        "            inp_vec = inp_vec[None, None, :]\n",
        "            pred_vec, hidden = self.forward(inp_vec.float())\n",
        "\n",
        "            pred_id = torch.multinomial(torch.nn.functional.softmax(\n",
        "                pred_vec, dim=-1)[0, 0, :], num_samples=1)\n",
        "            if verbose:\n",
        "                print(\"Predicted ID:\", pred_id)\n",
        "\n",
        "            pred_char = id_to_char[pred_id.item()]\n",
        "            input = pred_char\n",
        "            result += input\n",
        "            if verbose:\n",
        "                print(\"Current result:\", result)\n",
        "\n",
        "        return result\n",
        "\n",
        "# %%\n",
        "model = Model(len(id_to_char), len(id_to_char), 10, 1).to(device)\n",
        "model.predict(\"abc\", 10)\n",
        "\n",
        "# %%\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# %%\n",
        "sample = next(iter(dataloader))\n",
        "a, b = sample\n",
        "# a\n",
        "\n",
        "# %%\n",
        "EPOCHS = 50\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    for i, batch in tqdm(enumerate(iter(dataloader))):\n",
        "        optimizer.zero_grad()\n",
        "        inp, label = batch\n",
        "        # print(\"Input before one-hot:\", inp.shape)\n",
        "        inp = torch.nn.functional.one_hot(inp, num_classes=len(char_to_id))\n",
        "        # print(\"Input after one-hot:\", inp.shape)\n",
        "        pred, hidden = model(inp.float())\n",
        "        # pred = pred.squeeze(-1)\n",
        "        # print(\"Label shape:\", label.shape) # [N/batch_size, L]\n",
        "        # print(\"Pred shape:\", pred.shape) # [N/batch_size, L, num_classes]\n",
        "        label = label.view(-1)\n",
        "        pred = pred.view(-1, pred.size(2))\n",
        "        # print(\"Pred shape:\", pred.shape) # [N/batch_size, L, num_classes]\n",
        "\n",
        "        # Compute loss and backpropagate\n",
        "        loss = loss_fn(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if epoch % 2 == 0:\n",
        "        print(model.predict(\"We are\", 30))\n",
        "\n",
        "# %%\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kfF4VxUwhWo1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}